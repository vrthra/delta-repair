\documentclass[sigconf,review,anonymous]{acmart}


\usepackage{xspace}
\usepackage{array,multirow,graphicx}
\usepackage{float}
\usepackage{framed}
\usepackage{tikz,pgfplots,pgfplotstable}
\usetikzlibrary{calc,positioning,patterns}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{boxedminipage}
\usepackage[inline]{enumitem}
\setlength{\FrameSep}{3pt}
\setlength{\OuterFrameSep}{2pt}
\newenvironment{result}{\begin{framed}\centering\it}{\end{framed}}


\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO: }\emph{#1}}}
\newcommand{\com}[1]{\textcolor{orange}{\textbf{COMMENT: }\emph{#1}}}
\newcommand{\recheck}[1]{\textcolor{red}{#1}}
\newcommand{\revise}[1]{\textcolor{blue}{#1}}


\newcommand{\approach}{\textsc{Approach}\xspace}
\def\bfr{bFuzzerRepairer\xspace}
\def\ddmin{DDMin\xspace}
\newcommand{\ddmax}{\textit{ddmax}\xspace}
\newcommand{\ddmaxg}{\textit{ddmaxg}\xspace}
\newcommand{\bsimple}{\textit{bsimple}\xspace}
\newcommand{\brepair}{\textit{brepair}\xspace}

\usepackage{pifont}
\newcommand{\pass}{\text{\ding{52}}\xspace}
\newcommand{\fail}{\text{\ding{56}}\xspace}

\tikzset{inlinenode/.style={draw=white,text=black,fill=light-gray,inner sep=.1em,outer sep=0em}}
\newcommand{\inlinenode}[1]{\text{\hspace{.1em}\tikz[baseline=(n.base)]{\node[inlinenode] (n) {\strut \hspace*{.3em}#1\hspace*{.3em}};}\hspace{.1em}}}
\newcommand{\inlinetext}[1]{\text{\hspace{.1em}\tikz[baseline=(n.base)]{\node[inlinenode] (n) {\strut \hspace*{.3em}\letterboxed{#1}\hspace*{.3em}};}\hspace{.1em}}}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.3pt] (char) {#1};}}

\tikzset{%
simpletext/.style={draw=none,text=black,font=\normalfont\normalsize,align=center},
gparsetreenode/.style={minimum width=6mm,minimum height=5mm},
lparsetreenode/.style={gparsetreenode,simpletext,rectangle,draw=white,fill=white,align=center},
lparsetreeerrornode/.style={lparsetreenode,font=\bfseries},
lparsetreephantomnode/.style={gparsetreenode,edge from parent/.append style={draw=none},shape=coordinate,minimum width=15mm},
lparsetreestrikethrough/.style={draw=black,thick},
lparsetreedeletednode/.style={lparsetreenode,append after command={\pgfextra \draw[lparsetreestrikethrough] (\tikzlastnode.north west) -- (\tikzlastnode.south east); \draw[lparsetreestrikethrough] (\tikzlastnode.north east) -- (\tikzlastnode.south west);\endpgfextra}},
lparsetree/.style={node distance=5mm,level distance=10mm,every node/.style={lparsetreenode},edge from parent/.style={draw=black,-latex,shorten >=.5mm}
},
lflowchartnode/.style={lparsetreenode,draw=black,rounded corners=.5pt},
blockdiagramlines/.style={draw,stroke=black,line width=1.2pt},
blockdiagramarrow/.style={blockdiagramlines,->},
blockdiagramdashedarrow/.style={blockdiagramarrow,dashed},
blockdiagramannot/.style={blockdiagramlines,text=black,align=center},
blockdiagramblock/.style={lflowchartnode,blockdiagramannot,minimum width=1.5cm,minimum height=0.5cm,text width=1.9cm},
blockdiagrammicroblock/.style={blockdiagramannot,font=\tiny,minimum width=1.5cm,minimum height=.5cm,rounded corners=.5pt},
blockdiagramarrowcaption/.style={font=\scriptsize\sffamily,inner sep=1.5pt,text=black},
blockdiagramouterbox/.style={blockdiagramlines,densely dotted,line width=.7pt},
blockdiagramouterboxcaption/.style={blockdiagramarrowcaption,font=\itshape\scriptsize,inner sep=1pt},
pics/numbering/.style args={#1}{code={
    \node[draw=black,shape=circle,fill=white,text=black,font=\ttfamily\scriptsize,inner sep=1pt,text width=8pt,align=center,outer sep=0] (-number) {#1};
}}}

% Boxes around letters
\definecolor{light-gray}{gray}{0.87}
\makeatletter
\newcommand\letterboxed[1]{%
\setlength{\fboxsep}{0pt}%
  \@tfor\@ii:=#1\do{%
    \fcolorbox{white}{light-gray}{\texttt{\strut\@ii}}%
  }%
}
\makeatother


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[ESEC/FSE 2022]{The 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{14 - 18 November, 2022}{Singapore}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}

\usepackage[utf8]{inputenc}

\begin{document}

\title{Input Debugging via Rich %and Fast 
Failure Feedback}


\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
  \postcode{43017-6221}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \streetaddress{Rono-Hills}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \streetaddress{30 Shuangqing Rd}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \streetaddress{8600 Datapoint Drive}
  \city{San Antonio}
  \state{Texas}
  \country{USA}
  \postcode{78229}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

\date{February 2022}

\begin{abstract}
%Problem/
%\textbf{Context:} 
%\revise{
Program failures are sometimes induced by %when 
\textit{faulty inputs}, rather than \textit{buggy programs}, % are fed to \textit{valid} programs, 
e.g., 
%This can be 
due to incomplete or corrupted data. When an input \textit{solely} %is responsible for the program 
induces a failure in a valid program, the developer is saddled with the task of 
\textit{input debugging}.  %}
%\textbf{Objective:}  
%\revise{
Debugging \textit{failure-inducing inputs} involves identifying the  \textit{root cause} of the failure (e.g., the faulty input fragment)
% causing the failure), 
and \textit{repairing the input} such that it can be processed by the program. This is particularly difficult for structured inputs with complex input specifications (e.g., JSON).  
%} 
%\textbf{Methodology:} 
%\revise{
In this work, we present a \textit{language-agnostic, black-box} testing approach (called \approach) that leverages 
%ich and fast 
\textit{failure feedback} %from the program 
to automatically debug faulty inputs. The key insight of our approach %(called \approach) 
is to \textit{semantically} repair faulty inputs 
%to debug inputs via %by employing %a 
%combination of 
%\textit{test experimentation} that leverages 
using %leverage 
the richness of %and speed of 
\textit{failure feedback}, %to \textit{semantically}, 
such as 
%Specifically, \approach uses program feedback to 
%%. \approach employs  to identify %ing input properties such as the 
%identify 
the \textit{validity, incompleteness and incorrectness} checks of input fragments. 
Given a failure-inducing input and a valid program,  
\approach performs %conducts %test experiments 
semantic checks of input subsets to identify %ies 
faulty input fragments. 
%by conducting semantic checks of input fragments (e.g., (in)completeness) using test experiments. 
It then repairs the faulty input by removing or \textit{synthesizing} candidate input elements. % for repair.  
\revise{In our evaluation, \todo{\approach repaired ... recovered  ...} 
In addition, \approach overcomes the span length issue of (lexical) DDmax and outperforms DDMax by X\%, %despite %   %} In addition, \approach addresses several limitations of the state of the art, e.g., it overcomes the span length issue of (lexicial) DDmax and it does 
without access to an input grammar (syntactic DDmax). }

%\textbf{Conclusion:}


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{input debugging, input repair, program failures, structured inputs}


%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% TODO ...

One example for such an algorithm that localizes faults in failure-inducing inputs is \emph{Delta-Debugging}~\cite{Zeller:2002:SIF:506201.506206}.
It minimizes an input that induces a failure in a subject program while the minimized inputs still trigger the same bug.
In a similar manner, \emph{DDMax}~\cite{kirschner2020debugging} is a modification of the Delta-Debugging algorithm that, instead of minimizing test cases, maximizes failure-inducing input files while not triggering failures in the subject program.

A problem of DDMax is that it only supports deleting parts of the file under test while repairing the file.
Especially for highly-structured input formats like the JSON file seen in \Cref{fig:bad-json-input}, it is often the case that by inserting a simple character in the file, more information can be recovered in the file than by just deleting characters.

The \emph{bFuzzer} algorithm~\cite{gopinath2020fuzzing} is an input generator that determines valid continuations for input prefixes by making use of rich failure feedback.
The subject program does not only give feedback whether an input is accepted or not, but also whether the input is incorrect or incomplete.
Based on this input generation algorithm, \bfr makes use of this rich failure feedback to allow both insertions and deletions at the fault location.
In the example seen in \Cref{fig:bad-json-input-repaired}, \bfr first determines a valid (incomplete) prefix of the input file, i.e., \texttt{\{ "name": "Dave"}.
It then tries to append characters such that the prefix stays valid - in this example, \letterboxed{\}} and \letterboxed{,} are tried to be inserted.
Then, as much of the original file is appended to the prefix, until it becomes valid (and the rest of the original file is either appended, or deleted successively).
This results in the final repaired file seen in \Cref{fig:bad-json-input-repaired}.

%---
% Example for Insertion
%---
\begin{figure}[t]
\begin{center}
\letterboxed{\{\ "name":\ "Dave"\ "age":\ 42\ \}}
\end{center}
%\vspace{-0.6\baselineskip}
\caption{Failing JSON input}
\label{fig:bad-json-input}
\end{figure}
\begin{figure}[t]
\begin{center}
\letterboxed{\{\ "name":\ "Daveage"\ \}}
\end{center}
%\vspace{-0.6\baselineskip}
\caption{Failing input repaired with DDMax}
\label{fig:bad-json-input-ddmax}
\end{figure}
\begin{figure}[t]
\begin{center}
\letterboxed{\{\ "name":\ "Dave",\ "age":\ 42\ \}}
\end{center}
%\vspace{-0.6\baselineskip}
\caption{Failing input repaired with \bfr}
\label{fig:bad-json-input-repaired}
\end{figure}
\begin{figure}[t]
\begin{center}
\letterboxed{,}
\end{center}
%\vspace{-0.6\baselineskip}
\caption{Difference between failing and repaired input}
\label{fig:bad-json-input-diff}
\end{figure}

%TODO ...
Another problem occurring with DDMax is that it works very inefficient on inputs where the failure-inducing characters span a certain range in the file.% TODO which range?
Consider the file shown in \Cref{fig:bad-json-input-asterisks} -- a JSON object where the number \letterboxed{3.45} is preceded by three asterisks, rendering the file corrupted.
This example is similar to the one chosen in the introduction of \emph{Debugging Inputs}!\cite{kirschner2020debugging}.
% 37 -> 18 -> 9 -> 4
% { "item": "Apple", "price": ***3.45 }
%                   ->
%          ->       ->       ->
%     ->  ->  ->  ->  ->  ->  ->  ->   
When repairing this file, DDMax would first split the file in half, then in quarters and in eigths, finally trying to remove \letterboxed{***3} and \letterboxed{.45 \}}.
Those attempts do not succeed, so it tries to remove \letterboxed{**} and \letterboxed{*3} which also fails.
%TODO elaborate/rewrite?

With \bfr, repairing the file results in the file seen in \Cref{fig:bad-json-input-repaired-asterisks} -- \bfr first detects the fauklt to be at the leftmost asterisk.
It then tries to insert characters (a \letterboxed{"} and a \letterboxed{ } which are both valid continuations of the prefix) and delete one asterisk.
In the alternative where the \letterboxed{"} was inserted, \letterboxed{***3.45\ } can successfully be inserted from the original file.
Then, \bfr tries both inserting the \letterboxed{\}} from the original file and another \letterboxed{"} which succeeds and results in the repaired file.

%---
% Example for Span Problem
%---
\begin{figure}[t]
\begin{center}
\letterboxed{\{\ "item":\ "Apple",\ "price":\ ***3.45\ \}}
\end{center}
%\vspace{-0.6\baselineskip}
\caption{Failing JSON input that cannot be efficiently repaired with DDMax}
\label{fig:bad-json-input-asterisks}
\end{figure}
\begin{figure}[t]
\begin{center}
\letterboxed{\{\ "item":\ "Apple",\ "price":\ "***3.45\ "\}}
\end{center}
%\vspace{-0.6\baselineskip}
\caption{Failing input repaired with \bfr}
\label{fig:bad-json-input-repaired-asterisks}
\end{figure}

%TODO
\section{Overview}

\section{Approach}

\section{Experimental Setup}
\label{sec:experimental-setup}

%TODO This is copied from ddmax, change?
\begin{figure*}[t]
\newlength\nodedst\setlength\nodedst{.5cm}
\newlength\ndist\setlength\ndist{.1\nodedst}
\def\arrowsep{0.12cm}
\begin{tikzpicture}[node distance=\nodedst]
    \node[blockdiagramblock] (correct) {Input is correct?};
    \node[simpletext,below=of correct] (return) {Return input};
    \node[blockdiagramblock,right=of correct,font=\normalfont\scriptsize] (search) {Search longest incomplete prefix};
    \draw[blockdiagramarrow] (correct) -- (return) node[simpletext,pos=.4,right] {\pass};
    \draw[blockdiagramarrow] (correct) -- (search) node[simpletext,pos=.4,above] {\fail};
    %TODO
\end{tikzpicture}
% \vspace*{-0.1in}
    \caption{Workflow of the evaluation}\label{fig:ddmax_flowchart}
    % \vspace*{-0.1in}
\end{figure*}
\newcommand{\refnumber}[1]{\hyperref[fig:ddmax_flowchart]{Step~#1}}


This section describes the experimental setup of this work. 

\subsubsection*{\bf Workflow}
\Cref{fig:ddmax_flowchart} shows the workflow of our evaluation.
We first crawl a large set of real-world invalid files and select seed files according to \Cref{sec:prevalence}.
The seed files are filtered into valid and invalid files (\refnumber{1}).
Duplicates and files that have a wrong format are deleted beforehand.
Then, for each format, 50 valid \textit{crawled files} are selected and mutated using one mutation and up to 16 mutations to produce two additional sets of corrupted input files (\refnumber{2}).
We feed each invalid file to each subject program, and an \textit{ANTLR} parser (\refnumber{3}).
\textit{ANTLR} executes its default \textit{error recovery strategy} to recover from errors while generating a parse tree for the input.
Next, we feed the invalid file to the repair algorithms, i.e., \textit{lexical} \ddmax, \textit{syntactic} \ddmax and the \brepair and \bsimple algorithms (\refnumber{4}).
The repair algorithms test the input under repair repeatedly using the rich failure feedback from the subject program (\refnumber{5}).
Then, we feed the unmutated original files and the resulting repaired file from each technique to the \textit{differencing} framework (\refnumber{6}), which computes the \textit{change in file size} and \textit{Levenshtein distance} for both files.
We also save the feedback from the subject program (\refnumber{7}).
Finally, we execute \ddmin on the real-world invalid inputs (\refnumber{8}) and report the content and size of the result.

We implemented the repair algorithms in 2.1k~LOC of  \texttt{Java} code.
\textit{ANTLR} also implements an inbuilt error recovery strategy which is designed to recover from lexing or parsing errors (e.g. missing/wrong tokens or incomplete parse trees)~\cite{antlrjavadocdefaulterrorstrategy}.
%The real-world corrupted files are treated differently from the artificially mutated files, because we do not have working reference files for each corrupted file (i.e. the unmutated version of each file). We feed the corrupted real-world files into our \textit{lexical \ddmax} and evaluate the change in file size (i.e. the data loss on byte-level). We cannot evaluate the other metrics without unmutated files.

%TODO This is copied from ddmax, -->change?:
\subsubsection*{\bf Mutations}
In addition to the real-world invalid inputs (\textit{cf. \Cref{sec:prevalence}}), we also simulate real-world data corruption by applying \textit{byte-level} mutations on valid input files.
These mutations were chosen because they are similar to the corruptions observed in real-world invalid files (\textit{see \Cref{sec:prevalence} and \Cref{sec:diagnosis}}).
We perform the following mutations at a random position in each valid input file: \textit{byte insertion}, \textit{byte deletion} and \textit{byte flip}.
To simulate \textit{single data corruption}, we randomly choose one of these mutations and apply it once on the valid input file.
For \textit{multiple data corruptions}, we perform up to 16~random mutations on each input file.
A mutation is only successful (for an input format), if \textit{at least} one of the subject programs (that passes before) fails after the mutation.
These criteria are similar to how we collected invalid input files in the wild.

\subsubsection*{\bf Metrics and Measures}
To determine the quality of repaired files, we use the following metrics and tools:
\begin{enumerate}
    \item \textbf{File Size Difference: } We evaluate the difference in \textit{file size} of the \emph{recovered inputs} of the different repair algorithms and the original \textit{valid input}.
    We use these measurements to account for the amount of data recovered as well as the amount of data loss incurred.
\item \textbf{Levenshtein Distance: } Additionally, we measure the data loss between \textit{valid input} and \textit{repaired file} using the Levenshtein distance~\cite{levDistance}.
\end{enumerate}

\subsubsection*{\bf Research Protocol}
For each input format, we collect real-world invalid input files.
We also collect 50 valid real-world inputs and mutate those into a set with single and up to 16 multiple mutations.
Then, we execute all files on the subject programs, in order to determine the number of input files which fail for each subject program.
We proceed to run the repair algorithms on each invalid or mutated input file.
In particular, we are interested in determining the following:
%\begin{enumerate}
%\item[a] 
(1.) \textit{\textbf{Baseline:}} the number of invalid input files which are accepted by a subject program as \textit{valid inputs} (i.e. non-failure-inducing inputs processed by the program without leading to a crash), in order to measure the \textit{effectiveness} of the built-in \textit{error recovery} feature of the program;  and
%\item[b]  
(2.) \textit{\textbf{ANTLR:}} the number of invalid inputs which are repaired by ANTLR inbuilt \textit{error recovery strategy};
%\item[c]  
(3) \textit{\textbf{bRepair:}} the number of invalid inputs which are repaired by \brepair.

We conducted the experiments on a Dell Precision 7510 
with four physical CPU cores %(8~virtual cores) 
and 32GB of RAM, specifically an Intel(R) Core i7 6820hq @ 2.70GHz, 8~virtual cores, running 64-bit Arch Linux.
%All our prototypes are single-threaded.


\smallskip\noindent
\textbf{Research Questions: } 
In this paper, %addresses the following
we investigate the effectiveness, data recovery, diagnostic quality and efficiency of our approach via several experiments. For each experiment, %as well as,
%and 
we also examine how \approach compares to the state-of-the-art techniques, namely \textit{baseline} (built-in repair of subject programs), \textit{ANTLR}'s error recovery strategy, lexical ddmax and syntactic ddmax.  
%Particularly, 
To this end, we pose the following \textit{research questions} (RQ):
% concerning the performance of \approach:

\noindent
\textbf{RQ1 Effectiveness:}  %\& Efficiency:} 
How effective is our approach (\approach) in repairing invalid %failure-inducing 
inputs, and how does it compare to the state-of-the-art methods? 
%How effective is our approach, i.e., what is the time performance of \approach ?

\noindent
\textbf{RQ2 Data Recovery \& Loss:} 
%\revise{
How much input data is recovered by \approach, and how much input data loss is incurred?  Does it recover as much data as the 
%the data recovery of \approach compare to 
the state-of-the-art methods?
%}

%\noindent
%\textbf{RQ3 Efficiency:} 
%%\revise{
%How effective is our approach, i.e., what is the time performance of \approach ?
%%}

\noindent
\textbf{RQ3 Diagnostic Quality:} 
%\revise{
How effective is our approach in identifying the \textit{root cause} of invalid inputs, especially in comparison to the state-of-the-art methods (DDmin and DDmax)?
%}
%
%\noindent
%\textbf{RQ4 Comparison to the state-of-the-art:} 
%%\revise{
%How does \approach compare to the state-of-the-art input debugging techniques, in terms of effectiveness, efficiency, data recovery and diagnosis?
%%i.e., (DDMax \recheck{and Ddmin})?}
%\\

\noindent
\textbf{RQ4 Efficiency:} %ectiveness:} 
%\revise{
What is the efficiency (time performance) of 
%How does 
\approach? Is it as effective as the 
%especially in comparison %compare 
%to 
the state-of-the-art %input debugging 
techniques?
%, in terms of effectiveness, efficiency, data recovery and diagnosis?
%i.e., (DDMax \recheck{and Ddmin})?}


\noindent
\textbf{State-of-the-art:} 
In this work, we compare the performance of our approach to the state-of-the-art techniques. Specifically, we compare to (a) the built-in error-recovery technique of the subject programs (\textit{baseline}), (b) the inbuilt error recovery strategy of the ANTLR parser generator (\textit{ANTLR})~\cite{}, (c) the minimizing variant of delta debugging (DDMin)~\cite{} and (d) three variants of the maximizing variant of delta debugging (DDMax)~\cite{kirschner2020debugging}.

\todo{cite and create command shortcuts for the state-of-the-art}

\noindent \textit{Baseline:}

\noindent \textit{ANTLR:}

\noindent \textit{DDMin:}

\noindent \textit{DDMax:}




\noindent
\textbf{Subject Programs and Commits:}


\noindent
\textbf{Metrics and Measure}

\noindent
\textbf{Implementation Details and Platform:}

\noindent
\textbf{Research Protocol:} 



\begin{table}[!tbp]\centering
\caption{Details of Failure-inducing Inputs}
\begin{tabular}{|l | r | r | r | r |}
\hline
&  \multicolumn{4}{c|}{\textbf{Number of Invalid Inputs}}  \\
\textbf{Type of Invalid Inputs} & \textbf{INI} & \textbf{cJSON} & \textbf{SExp} & \textbf{TinyC} \\
\hline
\textbf{Real-World Inputs} & 101 & 107 & 50 & 148 \\
\textbf{Single Mutation} & 50 & 50 & 50 & 50 \\
\textbf{Multiple Mutations} & 50 & 50 & 50 & 50 \\
\hline
\textbf{Total } (806) & 201 & 207 & 150 & 248 \\
\hline
\end{tabular}
\label{tab:input-details}
%\vspace{-0.5cm}
\end{table}


\begin{table}[!tbp]\centering
\caption{Effectiveness of \approach vs. the state-of-the-art}
%\vspace{-0.5cm} 
\begin{tabular}{|l | c | r  r  r  r |}
\hline
&  \multicolumn{5}{c|}{\textbf{Number of Repaired Inputs}}  \\
&  \multicolumn{1}{c|}{\textbf{ALL}} & \multicolumn{4}{c|}{\textbf{Input Format}}  \\
\textbf{Techniques} & \textbf{Total} \textbf{(\%)} & \textbf{INI} & \textbf{cJSON} & \textbf{SExp} & \textbf{TinyC} \\
\hline
\textbf{Baseline}   & 33 (4\%) & 27	 & 6 &	0	& 0\\
\textbf{ANTLR} & 344 (43\%) & 133 & 69 & 96 &  46   \\
\textbf{Lex. ddmax} & 553 (69\%) & \textbf{201}  & 122  & 124 & 106    \\ 			
\textbf{Syn. ddmax} & \textbf{608} (\textbf{75\%}) & \textbf{201}  & \textbf{140}  & \textbf{148}  & \textbf{119}  \\ 	
\hline 
\textbf{\approach}  & \textit{ \textbf{650}} (\textbf{81\%}) & \textit{ \textbf{201}} & \textit{ \textbf{173}}  & 122  & \textit{ \textbf{154}} \\
\hline
\textbf{Improv. vs. Lex.} &  \textbf{18\%}  & 0\% & \textbf{42\%} & -2\% & \textbf{45\%} \\ 
\textbf{Improv. vs. Syn.} &  \textbf{7\%}  & 0\% & \textbf{24\%} & -18\% & \textbf{29\%} \\
\hline
%\textbf{Total \#Repairs} & 763  & 510  & 490  & 425 &  2188 \\ 			
%\hline
\end{tabular}
\label{tab:effectiveness}
%\vspace{-0.5cm}
\end{table}

\section{Experimental Results}

In this section, we 
discuss the %findings %
results 
of our empirical evaluations. 
% for each of the posed research questions. 

\noindent
\textbf{RQ1 Effectiveness:} 
We evaluate the effectiveness of our approach using 806 invalid input files belonging to four different input formats, namely INI, cJSON, SExp, and TinyC.  \autoref{tab:input-details} provides details of the %number of 
%real-world and mutated 
invalid inputs used for each input format. In this experiment, 
%and efficiency of our approach 
%Specifically, 
we measure the total number of files repaired by our approach, in comparison to 
%the number of repairs achieved % compare to %as well as 
four state-of-the-art techniques,  
%for all four input formats. We also compare the effectiveness of \approach to that of \recheck{four} state-of-the-art techniques, 
namely the built-in error-recovery of the subject program (called \textit{baseline}) and \textit{ANTLR}, and % as well as 
the two maximizing variants of delta debugging, i.e., \textit{lexical ddmax} and \textit{syntactic ddmax}.   \autoref{tab:effectiveness} and \autoref{fig:effectiveness} highlight the (overall) effectiveness of each approach %, as well as their effectiveness 
for each input format. 
% to determine the strengths %and complementarity 
%of our approach. 
% and the time taken to achieve this repair. 

%\noindent \textit{Effectiveness:} 
%\todo{tables and figures}

\noindent \textbf{\textit{Repair Effectiveness:}} In our evaluation, %we found that 
\textit{our approach (\approach) is highly effective in repairing invalid input files, it repaired about four out of every five invalid input}. \approach repaired about \recheck{81\% (650 out of 806)} of invalid inputs, and it is \textit{up to 20 times more effective than %outperformed all four 
the state-of-the art-techniques}. 
\autoref{tab:effectiveness} 
%illustrates these results. 
shows that for most (three out of four) input formats, our approach performed best (except for SExp).  \approach is up to 45\% and 29\% more effective than lexical and syntactic ddmax %for two input formats 
%(CJSON and TinyC), 
respectively.  
% and shows that for 
On one hand, \approach is about 20x as effective as the error recovery of the subject programs (33 repairs), and almost twice as (or 43\% more) effective than ANTLR's inbuilt error-recovery strategy (344 repairs). On the other hand, our approach outperformed the second most effective technique (\textit{ddmax}) by up to \recheck{18\%}. \approach is 18\% and seven percent more effective than lexical ddmax (69\%) and syntactic ddmax (75\%), respectively. 
In summary, these results suggest that %(a) \approach is highly effective in repairing invalid input files, (b) our approach more effective than the baselines, and (c) 
the \textit{combination of rich failure feedback and input synthesis} is important for the effective repair of invalid input files, this is evident by the effectiveness of \approach in comparison to the baselines.  

\begin{result}
\approach is highly effective in repairing invalid input files: %, and more effective than the state-of-the-art. 
It repaired four out of five (81\%) invalid inputs and it is up to 18\% more effective than the best baseline. 
\end{result}


\begin{table}[!tbp]\centering
\caption{Details of Repaired Invalid %Failure-inducing 
Inputs}
\begin{tabular}{|l | l | r | r | r | r | r | l |}
\hline
&  \multicolumn{5}{c|}{\textbf{Number of Repaired Inputs}}  \\
\textbf{Technique(s)} & \textbf{INI} & \textbf{cJSON} & \textbf{SExp} & \textbf{TinyC} & \textbf{Total (\%)} \\
\hline
\textbf{Lex. ddmax } & 0 & 3 & 0 & 3 & 6  (0.8\%) \\
\textbf{Syn. ddmax } & 0 & 6 & 23 & 12 & 41  (5.6\%)  \\
\textbf{\approach } & 0 & 27 & 0 & 63 & 90 (12.3\%)  \\
\hline
\textbf{Lex. $\cap$ Syn.} & 0 & 14 & 3 & 19 & 36  (4.9\%) \\
\textbf{Lex. $\cap$ Appr.} & 0 & 26 & 0 & 3 & 29  (4.0\%) \\
\textbf{Syn. $\cap$ Appr.} & 0 & 41 & 1 & 7 & 49  (6.7\%) \\
\hline
\textbf{All 3 techs.}
%Lex. $\cap$ Syn. $\cap$ Appr.} 
& 201 & 79 & 121 & 81 & 482  (65.8\%)\\
%\textbf{Both} &  201 & 105 & 121 & 84 & 511 (74\%) \\
%\textbf{Ddmax Only} & 0 & 17 & 3 & 22 & 42 (6\%) \\
%\textbf{\approach Only} & 0 & 68 & 1 & 70 & 139 (20\%) \\
\hline
\textbf{All Repairs} & 201 & 196 & 148 & 188 &  733  (N/A)\\
\hline
%\textbf{No Repairs} & 0 & 2 & 11 & 60 & 73 \\
%\hline
%\textbf{Total Repaired} & & & & & \\
%\textbf{Total Repaired} & 201 & 190 &  125 & 176 & 692  \\
%\hline
%\textbf{Total Unrepaired} &  0 & 17 & 25 & 72 & 114 \\
%\hline
\end{tabular}
\label{tab:repair-complementarity}
%\vspace{-0.5cm}
\end{table}



\begin{figure}[!tbp]
\vspace{-0.1cm}
\centering
%\includegraphics[width=0.45\textwidth]{figures/effectiveness.png}
\pgfplotstableread{
Label      INI    cJSON    SExpParser   TinyC
Baseline   27     6        0            0
ANTLR      133    69       96           46
Lex.ddmax  201    122      124          106
Syn.ddmax  201    140      148          119
BRepair    201    173      122          154
    }\effectivenessdata
\begin{minipage}{.45\textwidth}
\begin{tikzpicture}
\begin{axis}[
    ybar stacked,
    ymin=0,
    ymax=750,
    xtick=data,
    bar width=25,
    legend style={cells={anchor=west}, legend pos=north west},
    reverse legend=true,
    xticklabels from table={\effectivenessdata}{Label},
    xticklabel style={text width=2cm,align=center,font=\footnotesize},
]
    \addplot [pattern=north east lines] table [y=INI, meta=Label, x expr=\coordindex] {\effectivenessdata};
    \addlegendentry{INI}
    \addplot [pattern=grid] table [y=cJSON, meta=Label, x expr=\coordindex] {\effectivenessdata};
    \addlegendentry{cJSON}
    \addplot [pattern=north west lines] table [y=SExpParser, meta=Label, x expr=\coordindex] {\effectivenessdata};
    \addlegendentry{SExpParser}
    \addplot [pattern=dots,nodes near coords,point meta=y] table [y=TinyC, meta=Label, x expr=\coordindex] {\effectivenessdata};
    \addlegendentry{TinyC}
\end{axis}
\end{tikzpicture}
\end{minipage}
\caption{Number of files repaired by each approach}
\label{fig:effectiveness}
%\vspace{-0.6cm}
\end{figure}

\begin{figure}[!tbp]
\vspace{-0.4cm}
\centering
\begin{minipage}[b]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[circ/.style={draw=black,line width=1pt,fill=none,shape=circle,minimum width=2.5cm,minimum height=2.5cm},lbl/.style={font=\bfseries}]
        \node[draw=none,minimum width=1.5cm,minimum height=1.29904cm,inner sep=0,outer sep=0] at (0,0) (anchor) {};
        \node[circ] at (anchor.south west) (g1) {};
        \node[circ] at (anchor.south east) (g2) {};
        \node[circ] at (anchor.north) (g3) {};
        \node[lbl,anchor=north] at (g1.south) {\ddmax};
        \node[lbl,anchor=north] at (g2.south) {\ddmaxg};
        \node[lbl,anchor=south] at (g3.north) {\brepair};
        \node[lbl] at ($(g1)!0.5!(g2) - (0,.4)$) {36};
        \node[lbl] at ($(g2)!0.5!(g3) + (.2,.2)$) {49};
        \node[lbl] at ($(g1)!0.5!(g3) + (-.2,.2)$) {29};
        \node[lbl] at ($(anchor.center) - (0,.2)$) {482};
        \node[lbl] at ($(g1.center) - (.2,.1)$) {6};
        \node[lbl] at ($(g2.center) - (-.2,.1)$) {41};
        \node[lbl] at ($(g3.center) + (0,.2)$) {90};
    \end{tikzpicture}
\end{minipage}
\vspace{-0.4cm}
\caption{Venn Diagram showing the number of invalid inputs repaired (solely) by a (combinations of) technique(s) %, or the combination of approaches
}
\label{fig:repair-complementarity}
\vspace{-0.4cm}
\end{figure}

%\begin{figure}[!tbp]
%\vspace{-0.1cm}
%\centering
%%\includegraphics[width=0.45\textwidth]{images/perfect_bug_understanding}
%\includegraphics[width=0.5\textwidth]{figures/complementarity_pie_chart.png}
%\vspace{-0.6cm}
%\caption{XXX}
%\label{fig:repair_complementarity}
%\vspace{-0.6cm}
%\end{figure}

\noindent \textbf{\textit{Complementarity to Ddmax:}} We further inspect the unique repairs achieved by each approach, in order to understand the complementarity of our approach to the state-of-the-art methods. In this experiment, we inspect the number of unique repairs achieved \textit{solely} by a single approach (e.g., \textit{only} \approach), or two or more approaches (e.g., all approaches).  The goal is to determine if there are unique sets of inputs that are solely repaired by a single approach, and not by other approaches. \autoref{fig:repair-complementarity} and \autoref{tab:repair-complementarity} highlight our findings on the complementarity of \approach to ddmax. 
%to the state-of-the-art.
%demonstrate that our approach complements 
%the complementarity of our approach to 
%DDmax. 

In our evaluation, we found that about 12\% of all repairs could only be completed by \approach, which is is up to 15 times as many as DDmax repairs (\textit{see} \autoref{fig:repair-complementarity}). 
%our 
\approach \textit{solely} repaired more than 
%for %repairs 
\textit{one in every eight} invalid inputs (12\% of repairs), %\approach is the solely repaired them, 
%i.e., 
all of which ddmax could not repair. % these inputs. %. In addition, \approach \textit{solely} repairs 
%which 
%This is up to 15 times as many as DDmax repairs.
\autoref{tab:repair-complementarity} shows that for lexical ddmax, our approach \textit{solely} repaired 15x as many invalid files as lexical ddmax (90 versus six), while it repairs more than twice as many files as syntactic ddmax (90 versus 41). 
% \textit{one in five (20\% of) invalid inputs could be repaired solely by \approach}. \autoref{tab:repair-complementarity} and \autoref{fig:repair-complementarity} highlight the complementarity of \approach to ddmax. 
These findings show that our approach not only outperforms ddmax, but it is also complementary to ddmax: It solely repairs 90 (11\% of all) invalid inputs, all of which ddmax could not repair. %This is evident since it solely completes (12\% of) repairs that could not be completed by the state-of-the-art (ddmax). 
Further 
%experiment 
inspection suggests that this unique repairs achieved by \approach are due to the \textit{rich failure feedback} and \textit{repair via insertion} synergy of \approach. 
%insertion and 

% performance   


\begin{result}
%Our input repair 
\approach 
%not only outperforms the baseline, but 
outperforms and complements the best state-of-the-art method (ddmax).
%\approach 
It is the only technique that can solely complete 12\% of all repairs, which is up to 15 times as many as ddmax. 
\end{result}

\noindent
\textbf{RQ2 Data Recovery \& Loss:} 

\begin{result}

\end{result}


\noindent
\textbf{RQ3 Diagnostic Quality:} 

\begin{result}
XXXX
XXXX
\end{result}


%\noindent
%\textbf{RQ4 Comparison to the state-of-the-art:} 
%
%
%\noindent \textit{ Effectiveness \& Efficiency:}
%\begin{result}
%
%\end{result}
%%
%%\noindent \textit{Efficiency:}
%%\begin{result}
%%
%%\end{result}
%
%\noindent \textit{Data Recovery \& Loss:}
%
%\begin{result}
%
%\end{result}
%
%\noindent \textit{Diagnostic Quality:}
%
%\begin{result}
%
%\end{result}



\section{Discussion}

\section{Threats to Validity}
Our approach (\approach) and empirical evaluations may be limited by the following validity threats:

\noindent
\textbf{External Validity:}

\noindent
\textbf{Internal Validity:}


\noindent
\textbf{Construct Validity:}


\section{Related Work}
\label{sec:related_work}
%TODO In this chapter, we discuss...

\begin{description}

\item[Input Rectification] is the process of transforming misbehaving inputs into inputs that behave predictably in the scope of a certain software system.
    In \textit{Automatic Input Rectification}~\cite{Long:2012:AIR:2337223.2337233} and \emph{Living in the Comfort Zone}~\cite{Rinard:2007:LCZ:1297027.1297072}, input constraints are learned from valid inputs to solve this problem, which are used to transform malicious inputs into rectified inputs that satisfy learned constraints.
    We do not employ such constraint learning techniques in \bfr.
    Instead, we employ the knowledge of a grammar describing the input file format and the feedback of a subject program to transform the input files into an acceptable subset.
    Instead of transforming inputs to comply to security-critical constraints, our goal is to recover as much of the input file as possible.

\item[File Recovery] aims to recover files that cannot be opened with a subject program due to corruption.
In \emph{S-DAGs}~\cite{scheffczyk2004s}, semantic consistency constraints are enforced on broken input files in a semi-automatic technique.
\emph{Docovery}~\cite{docovery:ase14} takes a similar approach using symbolic execution to change broken inputs to take error-free paths in the subject program.
While this is a whitebox approach that facilitates program analysis, \bfr relies on the feedback of the subject program in a black-box approach that results from executing the broken inputs.

\item[Input Debugging], in contrast to program debugging, aims to change inputs to localize faults in subject programs.
    There is numerous work that focuses on simplifying failure-inducing inputs~\cite{Zeller:2002:SIF:506201.506206, clause2009penumbra, hierarchicalDD, sterling2007automated}.
    Closely related to \bfr are particularly \cite{hierarchicalDD} and \cite{sterling2007automated}, which both aim to minimize the input files while still triggering a bug in the subject program, just like \ddmin.
    In contrast, \bfr tries to maximize the inputs while avoiding to trigger the bug in the subject program.

\item[Data Diversity] \cite{data_diversity} transforms a failure-inducing input into an input that can be processed by a subject program while generating the same output.
Their approach analyzes which regions of the input cause the fault and changes those regions to avoid the fault.
An important difference to \bfr is that we do not need any program analysis, but analyze the output of the subject program alone.

\item[Data Structure Repair] iteratively fixes corrupted data structures by enforcing they conform to consistency constraints~\cite{Demsky:2003:ADR:949343.949314, 1553560, hussain2010dynamic, Demsky:2006:IED:1146238.1146266}.
These constraints can be extracted, specified and enforced with predicates~\cite{elkarablieh2008juzi}, model-based systems~\cite{Demsky:2003:ADR:949343.949314}, goal-directed reasoning~\cite{1553560}, dynamic symbolic execution~\cite{hussain2010dynamic} or invariants~\cite{Demsky:2006:IED:1146238.1146266}.
Similarly to our approach, one goal in data structure repair is to successfully execute a subject program on broken input files safely and acceptably.
However, \bfr focuses on repairing inputs while recovering as much data as possible to avoid inducing faults in the subject program.

\item[Syntactic Error Recovery] is the implementation of error recovery schemes, typically in parsers and compilers~\cite{hammond1984survey, backhouse1979syntax}.
In most of the syntactic recovery approaches, various different techniques are employed, including insertion, deletion and replacement of symbols~\cite{anderson1981locally, cerecke2003locally, anderson1983assessment}, extending forward or backwards from a parser error~\cite{burke1982practical, mauney1982forward}, or more general methods of recovery and diagnosis~\cite{krawczyk1980error, aho1972minimum}.
Different to \bfr, these schemes ensure the compiler does not halt while parsing, while our approach aims to fix the input inducing the failure.

\item[Data Cleaning and Repair] is typically performed on complex database systems.
In most approaches, this includes analyzing the database to remove noisy data or fill in missing data~\cite{xiong2006enhancing, hernandez1995merge}.
In other approaches, developers are allowed to write and apply logical rules on the database~\cite{pochampally2014fusing, galhardas2000ajax, golab2010data, jeffery2006pipelined, raman2001potter, luebbers2003systematic}.
While all of these approaches repair database systems, \bfr repairs raw user inputs.

\item[Data Testing and Debugging] aims to identify program errors caused by well-formed but incorrect data while a user modifies a database~\cite{mucslu2013data}.
In continuous data testing (CDT)~\cite{mucslu2015preventing}, likely data errors are identified by continuously executing domain-specfic test queries, in order to warn users of test failures.
\emph{DATAXRAY}~\cite{wang2015error} also investigates the underlying conditions that cause data bugs, it reveals hidden connections and common properties among data errors.
In contrast to \bfr, these approaches aim to guard data from new errors by detecting data errors in database systems during modification.
\end{description}

\section{Conclusion}
\todo{XXXX }

\revise{We provide our tool, data and experimental results for easy replication, scrutiny and reuse:
}

 \begin{center}
 \vspace{-0.2mm}
     \textbf{\url{https://github.com/XXX}}
 \end{center}
 
% \begin{acks}
% % To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}


\end{document}
